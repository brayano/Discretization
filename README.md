# Meshy Optimization

We consider the statistical problem where we measure a response <a href="https://www.codecogs.com/eqnedit.php?latex=y_i" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y_i" title="y_i" /></a>, and a covariate <a href="https://www.codecogs.com/eqnedit.php?latex=x_i&space;\in[0,1]" target="_blank"><img src="https://latex.codecogs.com/gif.latex?x_i&space;\in[0,1]" title="x_i \in[0,1]" /></a>, on each of n observations. We assume a generative model of the form <a href="https://www.codecogs.com/eqnedit.php?latex=y_i&space;=&space;f_0&space;\left(x_i&space;\right)&space;&plus;&space;w_i" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y_i&space;=&space;f_0&space;\left(x_i&space;\right)&space;&plus;&space;w_i" title="y_i = f_0 \left(x_i \right) + w_i" /></a>, where <a href="https://www.codecogs.com/eqnedit.php?latex=f_0" target="_blank"><img src="https://latex.codecogs.com/gif.latex?f_0" title="f_0" /></a> is an unknown function from a known function class <a href="https://www.codecogs.com/eqnedit.php?latex=\mathclass{F}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathclass{F}" title="\mathclass{F}" /></a>, and <a href="https://www.codecogs.com/eqnedit.php?latex=w_i" target="_blank"><img src="https://latex.codecogs.com/gif.latex?w_i" title="w_i" /></a> are iid errors with <a href="https://www.codecogs.com/eqnedit.php?latex=\operatorname{E}\left[w_i\right]&space;=&space;0" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\operatorname{E}\left[w_i\right]&space;=&space;0" title="\operatorname{E}\left[w_i\right] = 0" /></a> and <a href="https://www.codecogs.com/eqnedit.php?latex=\operatorname{var}{w_i}&space;=&space;\sigma^2&space;<&space;\infty" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\operatorname{var}{w_i}&space;=&space;\sigma^2&space;<&space;\infty" title="\operatorname{var}{w_i} = \sigma^2 < \infty" /></a>. We are interested in estimating <a href="https://www.codecogs.com/eqnedit.php?latex=f_0" target="_blank"><img src="https://latex.codecogs.com/gif.latex?f_0" title="f_0" /></a> based on the observed data. One common approach for estimating f is to use penalized regression:

	<a href="https://www.codecogs.com/eqnedit.php?latex=\hat{f}&space;=&space;\operatorname{argmin}_{f\in\mathcal{F}}\frac{1}{n}\sum_{i=1}^n\left(y_i&space;-&space;f\left(x_{i,\cdot}\right)\right)^2&space;&plus;&space;\lambda_n&space;P\left(f\right)," target="_blank"><img src="https://latex.codecogs.com/gif.latex?\hat{f}&space;=&space;\operatorname{argmin}_{f\in\mathcal{F}}\frac{1}{n}\sum_{i=1}^n\left(y_i&space;-&space;f\left(x_{i,\cdot}\right)\right)^2&space;&plus;&space;\lambda_n&space;P\left(f\right)," title="\hat{f} = \operatorname{argmin}_{f\in\mathcal{F}}\frac{1}{n}\sum_{i=1}^n\left(y_i - f\left(x_{i,\cdot}\right)\right)^2 + \lambda_n P\left(f\right)," /></a>

where <a href="https://www.codecogs.com/eqnedit.php?latex=\lambda_n&space;\geq&space;0" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\lambda_n&space;\geq&space;0" title="\lambda_n \geq 0" /></a> is a tuning parameter and <a href="https://www.codecogs.com/eqnedit.php?latex=P(\cdot)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(\cdot)" title="P(\cdot)" /></a> is a penalty function which penalizes ``complexity.''

In our framework, we alter the penalized regression problem slightly. We select a mesh of m knots over the domain of x, and use the fitted-values at those knots as our optimization parameters. We replace the penalty function with a finite-difference/Riemann approximation. Fitted values at data points are calculated by cleverly interpolating between fitted values at knots. For brevity, we refer to the general approach as 'meshy optimization.' 
